#!/usr/bin/env python3
"""
ç®€åŒ–ç‰ˆåŒ»ä¿SDKå‹åŠ›æµ‹è¯•
ä¸“æ³¨äºæ ¸å¿ƒåŠŸèƒ½æµ‹è¯•ï¼Œé¿å…å¤æ‚çš„ä¾èµ–é—®é¢˜
"""

import os
import sys
import time
import threading
import statistics
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from medical_insurance_sdk.core.database import DatabaseManager, DatabaseConfig
from medical_insurance_sdk.core.cache_manager import RedisCacheManager


def test_database_connection_pool():
    """æµ‹è¯•æ•°æ®åº“è¿æ¥æ± æ€§èƒ½"""
    print("=== æ•°æ®åº“è¿æ¥æ± æ€§èƒ½æµ‹è¯• ===")
    
    try:
        # åˆ›å»ºæ•°æ®åº“ç®¡ç†å™¨
        db_config = DatabaseConfig.from_env()
        db_manager = DatabaseManager(db_config)
        
        # æµ‹è¯•å‚æ•°
        concurrent_connections = 10
        operations_per_connection = 5
        
        response_times = []
        errors = []
        successful_requests = 0
        failed_requests = 0
        lock = threading.Lock()
        
        def db_worker(worker_id):
            """æ•°æ®åº“å·¥ä½œçº¿ç¨‹"""
            nonlocal successful_requests, failed_requests
            
            for i in range(operations_per_connection):
                start_time = time.time()
                try:
                    # æ‰§è¡Œç®€å•æŸ¥è¯¢
                    result = db_manager.execute_query("SELECT 1 as test_value")
                    
                    response_time = time.time() - start_time
                    with lock:
                        response_times.append(response_time)
                        successful_requests += 1
                    
                    time.sleep(0.01)  # çŸ­æš‚ä¼‘æ¯
                    
                except Exception as e:
                    response_time = time.time() - start_time
                    with lock:
                        response_times.append(response_time)
                        errors.append(f"Worker {worker_id}: {str(e)}")
                        failed_requests += 1
        
        # æ‰§è¡Œå¹¶å‘æµ‹è¯•
        start_time = time.time()
        
        with ThreadPoolExecutor(max_workers=concurrent_connections) as executor:
            futures = [executor.submit(db_worker, i) for i in range(concurrent_connections)]
            
            for future in as_completed(futures):
                try:
                    future.result()
                except Exception as e:
                    print(f"å·¥ä½œçº¿ç¨‹å¼‚å¸¸: {e}")
        
        total_time = time.time() - start_time
        total_requests = successful_requests + failed_requests
        
        # è®¡ç®—ç»Ÿè®¡æ•°æ®
        if response_times:
            avg_response_time = statistics.mean(response_times)
            min_response_time = min(response_times)
            max_response_time = max(response_times)
        else:
            avg_response_time = min_response_time = max_response_time = 0
        
        requests_per_second = total_requests / total_time if total_time > 0 else 0
        success_rate = (successful_requests / total_requests * 100) if total_requests > 0 else 0
        
        print(f"æ€»è¯·æ±‚æ•°: {total_requests}")
        print(f"æˆåŠŸè¯·æ±‚: {successful_requests}")
        print(f"å¤±è´¥è¯·æ±‚: {failed_requests}")
        print(f"æˆåŠŸç‡: {success_rate:.1f}%")
        print(f"æ€»è€—æ—¶: {total_time:.3f}ç§’")
        print(f"å¹³å‡å“åº”æ—¶é—´: {avg_response_time:.3f}ç§’")
        print(f"æœ€å°å“åº”æ—¶é—´: {min_response_time:.3f}ç§’")
        print(f"æœ€å¤§å“åº”æ—¶é—´: {max_response_time:.3f}ç§’")
        print(f"æ¯ç§’è¯·æ±‚æ•°: {requests_per_second:.1f} RPS")
        
        if errors:
            print(f"é”™è¯¯ç¤ºä¾‹: {errors[:3]}")
        
        # è·å–è¿æ¥æ± çŠ¶æ€
        pool_status = db_manager.get_pool_status()
        print(f"è¿æ¥æ± çŠ¶æ€: {pool_status}")
        
        return success_rate >= 80
        
    except Exception as e:
        print(f"æ•°æ®åº“è¿æ¥æ± æµ‹è¯•å¤±è´¥: {e}")
        return False


def test_cache_system():
    """æµ‹è¯•ç¼“å­˜ç³»ç»Ÿæ€§èƒ½"""
    print("\n=== ç¼“å­˜ç³»ç»Ÿæ€§èƒ½æµ‹è¯• ===")
    
    try:
        # åˆ›å»ºRedisç¼“å­˜ç®¡ç†å™¨
        cache_manager = RedisCacheManager(
            host='localhost',
            port=6379,
            db=1,
            default_ttl=300,
            key_prefix='stress_test:'
        )
        
        # æµ‹è¯•å‚æ•°
        concurrent_operations = 10
        operations_per_worker = 20
        
        response_times = []
        errors = []
        successful_requests = 0
        failed_requests = 0
        lock = threading.Lock()
        
        # é¢„å¡«å……ä¸€äº›æ•°æ®
        for i in range(50):
            cache_manager.set(f"test_key_{i}", f"test_value_{i}")
        
        def cache_worker(worker_id):
            """ç¼“å­˜å·¥ä½œçº¿ç¨‹"""
            nonlocal successful_requests, failed_requests
            
            for i in range(operations_per_worker):
                start_time = time.time()
                try:
                    # éšæœºé€‰æ‹©æ“ä½œ
                    import random
                    operation = random.choice(['get', 'set', 'get', 'get'])  # æ›´å¤šè¯»æ“ä½œ
                    
                    if operation == 'get':
                        key = f"test_key_{random.randint(0, 99)}"
                        value = cache_manager.get(key)
                    else:  # set
                        key = f"test_key_{worker_id}_{i}"
                        value = f"test_value_{worker_id}_{i}"
                        cache_manager.set(key, value)
                    
                    response_time = time.time() - start_time
                    with lock:
                        response_times.append(response_time)
                        successful_requests += 1
                    
                    time.sleep(0.001)  # å¾ˆçŸ­çš„ä¼‘æ¯
                    
                except Exception as e:
                    response_time = time.time() - start_time
                    with lock:
                        response_times.append(response_time)
                        errors.append(f"Worker {worker_id}: {str(e)}")
                        failed_requests += 1
        
        # æ‰§è¡Œå¹¶å‘æµ‹è¯•
        start_time = time.time()
        
        with ThreadPoolExecutor(max_workers=concurrent_operations) as executor:
            futures = [executor.submit(cache_worker, i) for i in range(concurrent_operations)]
            
            for future in as_completed(futures):
                try:
                    future.result()
                except Exception as e:
                    print(f"å·¥ä½œçº¿ç¨‹å¼‚å¸¸: {e}")
        
        total_time = time.time() - start_time
        total_requests = successful_requests + failed_requests
        
        # è®¡ç®—ç»Ÿè®¡æ•°æ®
        if response_times:
            avg_response_time = statistics.mean(response_times)
            min_response_time = min(response_times)
            max_response_time = max(response_times)
        else:
            avg_response_time = min_response_time = max_response_time = 0
        
        requests_per_second = total_requests / total_time if total_time > 0 else 0
        success_rate = (successful_requests / total_requests * 100) if total_requests > 0 else 0
        
        print(f"æ€»è¯·æ±‚æ•°: {total_requests}")
        print(f"æˆåŠŸè¯·æ±‚: {successful_requests}")
        print(f"å¤±è´¥è¯·æ±‚: {failed_requests}")
        print(f"æˆåŠŸç‡: {success_rate:.1f}%")
        print(f"æ€»è€—æ—¶: {total_time:.3f}ç§’")
        print(f"å¹³å‡å“åº”æ—¶é—´: {avg_response_time:.3f}ç§’")
        print(f"æœ€å°å“åº”æ—¶é—´: {min_response_time:.3f}ç§’")
        print(f"æœ€å¤§å“åº”æ—¶é—´: {max_response_time:.3f}ç§’")
        print(f"æ¯ç§’è¯·æ±‚æ•°: {requests_per_second:.1f} RPS")
        
        if errors:
            print(f"é”™è¯¯ç¤ºä¾‹: {errors[:3]}")
        
        # è·å–ç¼“å­˜ç»Ÿè®¡
        cache_stats = cache_manager.get_stats()
        print(f"ç¼“å­˜å‘½ä¸­ç‡: {cache_stats.get('hit_rate', 0):.1f}%")
        print(f"ç¼“å­˜å¥åº·çŠ¶æ€: {cache_stats.get('is_healthy', False)}")
        
        # æ¸…ç†æµ‹è¯•æ•°æ®
        cache_manager.clear_all()
        cache_manager.close()
        
        return success_rate >= 80
        
    except Exception as e:
        print(f"ç¼“å­˜ç³»ç»Ÿæµ‹è¯•å¤±è´¥: {e}")
        return False


def test_concurrent_database_operations():
    """æµ‹è¯•å¹¶å‘æ•°æ®åº“æ“ä½œ"""
    print("\n=== å¹¶å‘æ•°æ®åº“æ“ä½œæµ‹è¯• ===")
    
    try:
        db_config = DatabaseConfig.from_env()
        db_manager = DatabaseManager(db_config)
        
        # æµ‹è¯•å‚æ•°
        concurrent_users = 15
        operations_per_user = 3
        
        response_times = []
        errors = []
        successful_requests = 0
        failed_requests = 0
        lock = threading.Lock()
        
        # æµ‹è¯•æŸ¥è¯¢
        test_queries = [
            ("SELECT COUNT(*) as count FROM medical_interface_config", ()),
            ("SELECT api_code, api_name FROM medical_interface_config LIMIT 5", ()),
            ("SELECT org_code, org_name FROM medical_organization_config LIMIT 3", ()),
        ]
        
        def user_worker(user_id):
            """ç”¨æˆ·å·¥ä½œçº¿ç¨‹"""
            nonlocal successful_requests, failed_requests
            
            for op_id in range(operations_per_user):
                start_time = time.time()
                try:
                    # éšæœºé€‰æ‹©æŸ¥è¯¢
                    import random
                    query, params = random.choice(test_queries)
                    
                    result = db_manager.execute_query(query, params)
                    
                    response_time = time.time() - start_time
                    with lock:
                        response_times.append(response_time)
                        successful_requests += 1
                    
                    time.sleep(0.05)  # æ¨¡æ‹Ÿç”¨æˆ·æ€è€ƒæ—¶é—´
                    
                except Exception as e:
                    response_time = time.time() - start_time
                    with lock:
                        response_times.append(response_time)
                        errors.append(f"User {user_id}, Op {op_id}: {str(e)}")
                        failed_requests += 1
        
        # æ‰§è¡Œå¹¶å‘æµ‹è¯•
        start_time = time.time()
        
        with ThreadPoolExecutor(max_workers=concurrent_users) as executor:
            futures = [executor.submit(user_worker, i) for i in range(concurrent_users)]
            
            for future in as_completed(futures):
                try:
                    future.result()
                except Exception as e:
                    print(f"ç”¨æˆ·å·¥ä½œçº¿ç¨‹å¼‚å¸¸: {e}")
        
        total_time = time.time() - start_time
        total_requests = successful_requests + failed_requests
        
        # è®¡ç®—ç»Ÿè®¡æ•°æ®
        if response_times:
            avg_response_time = statistics.mean(response_times)
            sorted_times = sorted(response_times)
            p95_response_time = sorted_times[int(len(sorted_times) * 0.95)]
        else:
            avg_response_time = p95_response_time = 0
        
        requests_per_second = total_requests / total_time if total_time > 0 else 0
        success_rate = (successful_requests / total_requests * 100) if total_requests > 0 else 0
        
        print(f"æ€»è¯·æ±‚æ•°: {total_requests}")
        print(f"æˆåŠŸè¯·æ±‚: {successful_requests}")
        print(f"å¤±è´¥è¯·æ±‚: {failed_requests}")
        print(f"æˆåŠŸç‡: {success_rate:.1f}%")
        print(f"æ€»è€—æ—¶: {total_time:.3f}ç§’")
        print(f"å¹³å‡å“åº”æ—¶é—´: {avg_response_time:.3f}ç§’")
        print(f"P95å“åº”æ—¶é—´: {p95_response_time:.3f}ç§’")
        print(f"æ¯ç§’è¯·æ±‚æ•°: {requests_per_second:.1f} RPS")
        
        if errors:
            print(f"é”™è¯¯ç¤ºä¾‹: {errors[:3]}")
        
        return success_rate >= 80
        
    except Exception as e:
        print(f"å¹¶å‘æ•°æ®åº“æ“ä½œæµ‹è¯•å¤±è´¥: {e}")
        return False


def main():
    """ä¸»å‡½æ•°"""
    print("å¼€å§‹æ‰§è¡ŒåŒ»ä¿SDKå‹åŠ›æµ‹è¯•")
    print("=" * 50)
    
    results = []
    
    # 1. æ•°æ®åº“è¿æ¥æ± æ€§èƒ½æµ‹è¯•
    db_result = test_database_connection_pool()
    results.append(("æ•°æ®åº“è¿æ¥æ± ", db_result))
    
    # 2. ç¼“å­˜ç³»ç»Ÿæ€§èƒ½æµ‹è¯•
    cache_result = test_cache_system()
    results.append(("ç¼“å­˜ç³»ç»Ÿ", cache_result))
    
    # 3. å¹¶å‘æ•°æ®åº“æ“ä½œæµ‹è¯•
    concurrent_result = test_concurrent_database_operations()
    results.append(("å¹¶å‘æ•°æ®åº“æ“ä½œ", concurrent_result))
    
    # æ€»ç»“
    print("\n" + "=" * 50)
    print("å‹åŠ›æµ‹è¯•æ€»ç»“")
    print("=" * 50)
    
    passed_tests = 0
    total_tests = len(results)
    
    for test_name, result in results:
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"{test_name}: {status}")
        if result:
            passed_tests += 1
    
    overall_success_rate = (passed_tests / total_tests * 100) if total_tests > 0 else 0
    print(f"\næ•´ä½“é€šè¿‡ç‡: {passed_tests}/{total_tests} ({overall_success_rate:.1f}%)")
    
    if overall_success_rate >= 80:
        print("ğŸ‰ å‹åŠ›æµ‹è¯•æ•´ä½“é€šè¿‡ï¼ç³»ç»Ÿæ€§èƒ½è‰¯å¥½")
        return True
    else:
        print("ğŸ’¥ å‹åŠ›æµ‹è¯•æœªå®Œå…¨é€šè¿‡ï¼Œéœ€è¦ä¼˜åŒ–ç³»ç»Ÿæ€§èƒ½")
        return False


if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)